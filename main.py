from crawler import Crawler
from storage import CSVStorage
from config import OUTPUT_FILE,START_OFFSET,END_OFFSET,BATCH_STEP,MAX_WORKERS,DETAIL_URL_TEMPLATE
from parser import parse_list_page,parse_publish_time,parse_detail_page
from utils import extract_post_id_from_url,random_delay
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from logger import logger
from tqdm import tqdm

def fetch_single_detail(task_data):
    """ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè§£æå¤šä¸ªè®£å‘Šé¡µé¢"""
    crawler = Crawler()
    post_id, title, raw_time = task_data

    try:
        detail_html = crawler.fetch_detail_page(post_id)
        content = parse_detail_page(detail_html) if detail_html else ""
    except Exception as e:
        logger.error(f"âŒ Detail failed for post={post_id} ({type(e).__name__}): {e}")
        content = ""
    finally:
        crawler.__del__()

    URL = DETAIL_URL_TEMPLATE.format(post_id=post_id)
    pub_time = parse_publish_time(raw_time)

    random_delay(1, 3)

    return {
        "URL": URL,
        "Title": title,
        "Publish_Time": pub_time,
        "Content": content
    }

def main():
    start_time = time.time()
    crawler = Crawler()
    storage = CSVStorage(OUTPUT_FILE)
    batch_data = []
    BATCH_SIZE = 100

    for offset in range(START_OFFSET,END_OFFSET+1,BATCH_STEP):
        get_offset_data_time = time.time()
        logger.info(f"Fetching list page at offset {offset}...")
        try:
            # TODO æ·»åŠ é‡è¯•æœºåˆ¶
            list_html = crawler.fetch_list_page(offset)
            urls, titles, raw_times = parse_list_page(list_html)
        except Exception as e:
            logger.error(f"âš ï¸ Skip offset {offset}: {e}")
            continue

        # å‡†å¤‡ä»»åŠ¡å‚æ•°åˆ—è¡¨
        tasks_params = []
        for url,title,raw_time in zip(urls,titles,raw_times):
            post_id = extract_post_id_from_url(url)
            if post_id:
                tasks_params.append((post_id,title,raw_time)) # å­˜å®Œæ‰€æœ‰å¸–å­éœ€è¦çš„å‚æ•°å¼€å¯å¤šçº¿ç¨‹
        
        with (ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor):
            futures = [executor.submit(fetch_single_detail,task_params) for task_params in tasks_params]

            for future in tqdm(
                as_completed(futures),
                total=len(futures),
                desc=f"Processing Offset {offset:04d}",   # å¦‚ "Offset 0100"
                unit="detail",                  # å•ä½æ˜¾ç¤ºä¸º "detail"
                ncols=80,                       # è¿›åº¦æ¡å®½åº¦
                leave=True,                     # å®Œæˆåä¿ç•™æœ€åä¸€è¡Œ
                colour='Green'                  # è¿›åº¦æ¡é¢œè‰²ï¼šç»¿è‰²
            ):
                result = future.result()
                batch_data.append(result)

            # æ‰¹é‡ä¿å­˜
            if len(batch_data) >= BATCH_SIZE:
                storage.save_batch(batch_data)
                logger.info(f"âœ… Saved {len(batch_data)} items")
                batch_data = []
        logger.info(f"Finish This Offset:{offset} time taken: {round(time.time() - get_offset_data_time,3)} seconds")

    # ä¿å­˜å‰©ä½™
    if batch_data:
        storage.save_batch(batch_data)
        logger.info(f"âœ… Final batch saved")

    logger.info(f"Total time taken: {time.time() - start_time} seconds")
    logger.info("ğŸ‰ All done!")

if __name__ == "__main__":
    main()